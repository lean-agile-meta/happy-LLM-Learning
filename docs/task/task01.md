# Happy-LLM 学习笔记 Task01

Happy-LLM 是从零开始讲解 LLM 原理、并引导学习者亲手搭建、训练 LLM 的完整教程。

- 从 NLP 的基本研究方法出发，根据 LLM 的思路及原理逐层深入，依次为读者剖析 LLM 的架构基础和训练过程。
- 结合目前 LLM 领域最主流的代码框架，演练如何亲手搭建、训练一个 LLM。
- 包含 LLM 的理论基础、原理介绍和项目实战
- 包括 LLM 及 NLP 的核心思路剖析、公式解析与代码实战
- 帮助开发者深入理解并掌握 LLM 的基本原理与应用

## NLP-PLM-LLM

### 自然语言处理 NLP

- Natural Language Process，NLP
  - 宏伟目标是“让计算机理解和处理人类语言”。
- NLP 领域聚焦于人类书写的自然语言文本的处理、理解和生成。
  - 经历了符号主义阶段、统计学习阶段、深度学习阶段、预训练模型阶段到而今大模型阶段的多次变革。

### 预训练语言模型 PLM

- Pretrain Language Model，PLM
  - 核心价值在于预训练+迁移学习，解决了 NLP 任务标注数据稀缺的问题，并学习到了强大的通用语言表示。
  - 以 GPT、BERT 为代表的 PLM 是上一阶段 NLP 领域的核心研究成果
    - 以注意力机制为模型架构，
    - 通过预训练-微调的阶段思想通过在海量无监督文本上进行自监督预训练，
    - 实现了强大的自然语言理解能力
  - 缺点
    - 传统的 PLM 仍然依赖于一定量有监督数据进行下游任务微调，且在自然语言生成任务上性能还不尽如人意

### 大语言模型 LLM

- Large Language Model，LLM
  - 核心价值在于规模效应带来的涌现能力，使得模型能更通用、更灵活地处理复杂任务，甚至展现出类人的理解和生成能力。
  - 在 PLM 的基础上，通过大量扩大模型参数、预训练数据规模，并引入指令微调、人类反馈强化学习等手段实现的突破性成果。
  - 相较于传统 PLM，LLM 具备涌现能力，具有强大的上下文学习能力、指令理解能力和文本生成能力。
  - 优点
    - 在大模型阶段，NLP 研究者可以一定程度抛弃大量的监督数据标注工作，通过提供少量监督示例，LLM 即能在指定下游任务上达到媲美大规模微调 PLM 的性能。
    - 强大的指令理解能力与文本生成能力使 LLM 能够直接、高效、准确地响应用户指令，从而真正向通用人工智能的目标逼近。

### NLP-PLM-LLM 三者的关系

- NLP 包含 PLM 包含 LLM
  - NLP 是研究处理人类语言的整个学科领域
  - PLM 是 NLP 领域内，当前最核心、最成功的技术范式（基于深度学习、预训练+微调）。
  - LLM 是 PLM 范式下，模型规模和训练数据量达到超大规模（十亿/万亿参数）时的一个特定类别，具备更强的通用性和涌现能力。

## 能力

- 深入理解、掌握 LLM 原理，
- 能够动手应用、训练任意一个 LLM 的能力

## 章节分类

### 基础知识

- 第 1 章简单介绍 NLP 的基本任务和发展，为非 NLP 领域研究者提供参考；
- 第 2 章介绍 LLM 的基本架构——Transformer，包括原理介绍及代码实现，作为 LLM 最重要的理论基础；
- 第 3 章整体介绍经典的 PLM，包括 Encoder-Only、Encoder-Decoder 和 Decoder-Only 三种架构，也同时介绍了当前一些主流 LLM 的架构和思想；
- 第 4 章则正式进入 LLM 部分，详细介绍 LLM 的特点、能力和整体训练过程。

### 实战应用

- 第 5 章将带领读者基于 PyTorch 层亲手搭建一个 LLM，并实现预训练、有监督微调的全流程；
- 第 6 章将引入目前业界主流的 LLM 训练框架 Transformers，带领读者基于该框架快速、高效地实现 LLM 训练过程；
- 第 7 章则将介绍 基于 LLM 的各种应用，补全读者对 LLM 体系的认知，包括 LLM 的评测、检索增强生成（Retrieval-Augmented Generation，RAG）、智能体（Agent）的思想和简单实现。
